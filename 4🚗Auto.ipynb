{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<small><font color=gray>Notebook author: <a href=\"https://www.linkedin.com/in/olegmelnikov/\" target=\"_blank\">Oleg Melnikov</a> ¬©2021 onwards</font></small><hr style=\"margin:0;background-color:silver\">\n",
        "\n",
        "**[<font size=6>üöóAuto</font>](https://www.kaggle.com/competitions/13nov23hse-auto/rules)**. [**Instructions**](https://colab.research.google.com/drive/1riOGrE_Fv-yfIbM5V4pgJx4DWcd92cZr#scrollTo=ITaPDPIQEgXV) for running Colabs."
      ],
      "metadata": {
        "id": "Po1qcjD2lW4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<small>**(Optional) CONSENT.** <mark>[ X ]</mark> We consent to sharing our Colab (after the assignment ends) with other students/instructors for educational purposes. We understand that sharing is optional and this decision will not affect our grade in any way. <font color=gray><i>(If ok with sharing your Colab for educational purposes, leave \"X\" in the check box.)</i></font></small>"
      ],
      "metadata": {
        "id": "HB_RPHLJloqz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToKDfNMabFMF",
        "outputId": "a932b75d-91da-40fd-ca87-9807c9cc8ed0"
      },
      "source": [
        "%%time\n",
        "%reset -f\n",
        "from IPython.core.interactiveshell import InteractiveShell as IS; IS.ast_node_interactivity = \"all\"\n",
        "import pandas as pd, time, numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "ToCSV = lambda df, fname: df.round(2).to_csv(f'{fname}.csv', index_label='id') # rounds values to 2 decimals\n",
        "\n",
        "class Timer():\n",
        "  def __init__(self, lim:'RunTimeLimit'=60): self.t0, self.lim, _ = time.time(), lim, print('timer started')\n",
        "  def ShowTime(self):\n",
        "    msg = f'Runtime is {time.time()-self.t0:.0f} sec'\n",
        "    print(f'\\033[91m\\033[1m' + msg + f' > {self.lim} sec limit!!!\\033[0m' if (time.time()-self.t0-1) > self.lim else msg)\n",
        "\n",
        "np.set_printoptions(linewidth=10000, precision=4, edgeitems=20, suppress=True)\n",
        "pd.set_option('display.max_rows', 100, 'display.max_columns', 100, 'display.max_colwidth', 100, 'display.precision', 2, 'display.max_rows', 4)\n",
        "\n",
        "db = fetch_openml('BNG(auto_price)')   # load databunch (dictionary)\n",
        "tX = pd.DataFrame(db['data'], columns=db['feature_names'])\n",
        "tX.symboling = tX.symboling.astype('float')\n",
        "tX['price'] = db['target']\n",
        "YCols = ['city-mpg','highway-mpg','price']  # 3 targets\n",
        "tY = tX[YCols]\n",
        "tX.drop(YCols, axis=1, inplace=True)\n",
        "# tY = pd.Series(db['target'], name='price')\n",
        "tX, vX, tY, DO_NOT_USE = train_test_split(tX, tY, train_size=0.7, random_state=0, shuffle=True)\n",
        "ToCSV(DO_NOT_USE, 'testY')   # Students cannot use these test values\n",
        "# del DO_NOT_USE\n",
        "tX\n",
        "tY\n",
        "tmr = Timer() # runtime limit (in seconds). Add all of your code after the timer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "timer started\n",
            "CPU times: user 24.6 s, sys: 1.83 s, total: 26.5 s\n",
            "Wall time: 49.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tmr = Timer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxjQQnTAlsog",
        "outputId": "0119e237-841e-446b-80a1-6791475bbc5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "timer started\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr color=red>\n",
        "\n",
        "<font size=5>‚è≥</font> <strong><font color=orange size=5>Your Code, Documentation, Ideas and Timer - All Start Here...</font></strong>\n",
        "\n",
        "**Student's Section** (between ‚è≥ symbols): add your code and documentation here."
      ],
      "metadata": {
        "id": "_MPlcR1YSIY9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpyLNt3god0c"
      },
      "source": [
        "## **Task 1. Preprocessing Pipeline**\n",
        "\n",
        "Explain elements of your preprocessing pipeline i.e. feature engineering, subsampling, clustering, dimensionality reduction, etc.\n",
        "1. Why did you choose these elements? (Something in EDA, prior experience,...? Btw, EDA is not required)\n",
        "1. How do you evaluate the effectiveness of these elements?\n",
        "1. What else have you tried that worked or didn't?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30xYIFXAnaPE"
      },
      "source": [
        "**Student's answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGJRwzqHob4o"
      },
      "source": [
        "## **Task 2. Modeling Approach**\n",
        "Explain your modeling approach, i.e. ideas you tried and why you thought they would be helpful.\n",
        "\n",
        "1. How did these decisions guide you in modeling?\n",
        "1. How do you evaluate the effectiveness of these elements?\n",
        "1. What else have you tried that worked or didn't?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi6ZjgtWnb58"
      },
      "source": [
        "**Student's answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a baseline model that produces the result on Kaggle leaderboard (LB)."
      ],
      "metadata": {
        "id": "c_U8i_Erl0oE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA tX"
      ],
      "metadata": {
        "id": "_ZUoeap2epww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tXcp = tX\n",
        "vXcp = vX\n",
        "tYcp = tY\n",
        "\n",
        "'''TODO:\n",
        "1) Hyperparameters using CV\n",
        "2) Splines\n",
        "3) Remove some parameters (that are 0 and then train on 700,000)\n",
        " '''"
      ],
      "metadata": {
        "id": "M_9JqK4efB4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a24feee9-cb23-4e56-86af-83a901cbf924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TODO:\\n1) Hyperparameters using CV\\n2) Splines\\n3) Remove some parameters (that are 0 and then train on 700,000)\\n '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''EDA'''\n",
        "\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# def plot_histograms_boxplots(df, columns_to_explore):\n",
        "#     # Histograms for each column, arranged in two lines\n",
        "#     plt.figure(figsize=(20, 8))  # Adjust the size as needed\n",
        "#     for i, column in enumerate(columns_to_explore):\n",
        "#         plt.subplot(2, 6, i + 1)  # 6 columns per row, change if needed\n",
        "#         sns.histplot(df[column], kde=True)\n",
        "#         plt.title(f'Histogram of {column}')\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "#     # Boxplots for each column, arranged in two lines\n",
        "#     plt.figure(figsize=(20, 8))  # Adjust the size as needed\n",
        "#     for i, column in enumerate(columns_to_explore):\n",
        "#         plt.subplot(2, 6, i + 1)  # 6 columns per row, change if needed\n",
        "#         sns.boxplot(x=df[column])\n",
        "#         plt.title(f'Boxplot of {column}')\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# # Now, you can call this function with your dataframe and list of columns\n",
        "# # Example usage:\n",
        "# plot_histograms_boxplots(tXcp, tXcp.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "F-2yDzKUchVn",
        "outputId": "fc211ff0-828c-4daa-f4df-25b2e4fefa2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'EDA'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "independently"
      ],
      "metadata": {
        "id": "-FNh4dJTdp-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tX = tXcp[:250000]\n",
        "tY = tYcp[:250000]\n",
        "vX = vXcp\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler, PolynomialFeatures\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "import numpy as np\n",
        "\n",
        "cols_to_drop = ['x1 x4 x10', 'x16 x18^2', 'x2 x5 x8', 'x9 x13 x15', 'x3 x6 x7', 'x1 x2 x13', 'x12 x13 x15', 'x11 x13 x14', 'x10 x12 x15', 'x3 x5 x18', 'x13 x17 x18', 'x3 x15 x16', 'x14^2 x18', 'x4 x6 x17', 'x11^2 x16', 'x0 x6 x7', 'x9 x11^2', 'x11 x13 x15', 'x3 x4 x17', 'x16^2 x18', 'x2 x3^2', 'x2 x6 x12', 'x11 x12 x13', 'x9 x11 x18', 'x2 x3 x13', 'x7 x11 x15', 'x4 x7 x9', 'x3 x5 x16', 'x7^2 x12', 'x0 x2 x18', 'x1 x4 x7', 'x9 x14 x18', 'x10 x12 x18', 'x6 x9 x14', 'x0 x3 x11', 'x2 x9 x11', 'x4 x5 x6', 'x1 x6 x18', 'x8 x16 x18', 'x3 x4 x15', 'x2 x3 x17', 'x3 x6 x15', 'x0 x5 x11', 'x1 x3 x6', 'x2 x5 x10', 'x3 x5', 'x12 x13 x18', 'x13 x14 x18', 'x5 x9^2', 'x8 x14 x17', 'x1 x4 x17', 'x8^2 x12', 'x7 x10 x12', 'x8^2 x13', 'x1^2 x3', 'x5 x6 x7', 'x12 x16 x17', 'x8 x10^2', 'x0 x6', 'x2 x3 x12', 'x1 x16 x17', 'x3 x6 x13', 'x13^2 x16', 'x0 x1 x17', 'x1 x7^2', 'x2^2 x6', 'x8 x13 x15', 'x4 x12 x18', 'x5 x11 x17', 'x4 x18^2', 'x16 x17^2', 'x1 x3 x5', 'x0 x3', 'x2 x5^2', 'x3 x6 x16', 'x0 x6 x12', 'x0 x6 x11', 'x3 x4 x14', 'x9 x14 x16', 'x1 x3^2', 'x0 x2 x15', 'x15^2 x17', 'x8 x10 x11', 'x8 x13^2', 'x1 x3 x18', 'x0^2 x5', 'x8 x12 x16', 'x12 x15^2', 'x1 x3 x16', 'x11^3', 'x11 x13 x16', 'x1 x4 x14', 'x15^2 x16', 'x1 x4', 'x1 x3 x14', 'x0 x3 x8', 'x1 x6 x9', 'x10 x16^2', 'x1 x5 x9', 'x16^2 x17', 'x2^2 x3', 'x5 x13 x16', 'x0 x1 x9', 'x2 x5 x12', 'x0^2 x1', 'x0 x2 x16', 'x2 x4 x6', 'x0 x5 x15', 'x3 x9 x16', 'x5 x6 x11', 'x11 x12 x16', 'x9 x10 x11', 'x8 x11 x16', 'x9 x10^2', 'x2 x3 x18', 'x0 x9 x12', 'x0 x1 x11', 'x11 x17 x18', 'x1 x3 x13', 'x13 x15 x18', 'x12 x14 x16', 'x0 x4 x8', 'x2 x5 x7', 'x0 x1 x2', 'x5 x8 x11', 'x9 x14 x15', 'x12 x16^2', 'x3 x4 x16', 'x11^2 x17', 'x0 x6 x15', 'x8^2 x9', 'x2 x11 x16', 'x18^3', 'x4 x7 x14', 'x5 x10^2', 'x1 x2', 'x0 x3 x10', 'x5 x16^2', 'x1 x4 x5', 'x0 x3 x5', 'x2 x4 x7', 'x3 x4 x6', 'x10^2 x18', 'x9^2 x16', 'x1 x5 x17', 'x1 x4 x9', 'x8 x9 x16', 'x3 x10 x15', 'x0 x2 x5', 'x9^2 x12', 'x2 x5', 'x3 x4 x18', 'x8 x9 x14', 'x2 x6 x14', 'x2 x5 x18', 'x14 x16 x18', 'x7 x15 x17', 'x0 x1 x15', 'x2 x3', 'x12 x15 x16', 'x1 x6 x17', 'x7^2 x17', 'x0 x1 x10', 'x1 x3 x8', 'x2 x5 x11', 'x4 x6 x11', 'x1 x5', 'x2 x5 x17', 'x0 x2 x14', 'x0 x5 x13', 'x0 x6 x8', 'x2 x3 x5', 'x10 x15 x17', 'x14 x17 x18', 'x1^2 x5', 'x0^2 x2', 'x0 x1^2', 'x13 x17', 'x7 x10 x14', 'x1 x2 x10', 'x10 x14 x18', 'x2 x12 x17', 'x2 x4 x18', 'x7 x8^2', 'x1 x4 x13', 'x10 x13 x18', 'x8 x11 x18', 'x9 x14^2', 'x9 x13 x16', 'x1 x2 x6', 'x10 x15^2', 'x1 x2 x12', 'x2 x4 x17', 'x5 x8 x13', 'x1 x2 x8', 'x1 x5 x11', 'x10 x11 x14', 'x5 x9 x11', 'x14 x15^2', 'x3 x12 x13', 'x0 x3 x6', 'x0 x3 x16', 'x0^2 x3', 'x3 x6 x10', 'x2 x13 x15', 'x1 x5 x16', 'x11 x15', 'x1 x3 x17', 'x2 x3 x15', 'x4 x11 x17', 'x0 x2 x13', 'x2 x12^2', 'x4^2 x5', 'x3^2 x5', 'x4 x5 x17', 'x3 x18^2', 'x11 x15 x18', 'x2 x8 x9', 'x12 x18^2', 'x0 x3 x12', 'x3 x6 x14', 'x8^2 x16', 'x3^2 x4', 'x3 x5 x13', 'x7 x10 x16', 'x0 x6 x10', 'x1 x4 x12', 'x0 x2 x9', 'x3 x5 x9', 'x2 x9 x14', 'x10^2 x12', 'x0 x2 x7', 'x0 x4 x14', 'x0 x1 x6', 'x0 x4 x9', 'x0 x1 x3', 'x0 x2 x11', 'x5^2 x10', 'x8 x11 x14', 'x9 x11 x12', 'x2 x3 x4', 'x0 x5 x6', 'x2 x4 x13', 'x9 x11 x16', 'x0 x4 x10', 'x3 x4 x7', 'x10 x13 x16', 'x12 x16 x18', 'x1 x2 x17', 'x1^2 x2', 'x4 x5^2', 'x2 x4 x10', 'x3 x4 x10', 'x2 x3 x9', 'x8^2 x18', 'x0 x3^2', 'x1^2 x4', 'x9 x10 x17', 'x3 x16 x17', 'x12 x14^2', 'x0 x5 x10', 'x0 x11 x17', 'x6 x7 x14', 'x2 x3 x7', 'x0 x2 x12', 'x9^2 x13', 'x0 x5 x18', 'x4 x5 x18', 'x0 x6^2', 'x1 x6 x14', 'x3 x5 x7', 'x1 x5 x15', 'x3 x4 x5', 'x3 x5 x8', 'x3 x6 x18', 'x2 x4 x5', 'x5 x6 x14', 'x15^2 x18', 'x1 x3 x12', 'x14 x17^2', 'x0 x4 x13', 'x0 x5^2', 'x0 x3 x13', 'x10^2 x16', 'x6 x14', 'x11 x14 x17', 'x5^2 x6', 'x4 x6 x16', 'x3^2 x6', 'x4 x8 x10', 'x2 x5 x14', 'x11 x12 x17', 'x2 x6 x8', 'x0 x4 x12', 'x12 x15 x18', 'x0 x5 x14', 'x13 x18^2', 'x4 x14 x15', 'x1 x10 x11', 'x0 x4^2', 'x2 x11 x13', 'x0 x5 x8', 'x1 x6 x15', 'x0 x3 x9', 'x4^2 x6', 'x1 x8 x10', 'x1 x4 x6', 'x2 x18^2', 'x0 x2 x17', 'x1 x5 x6', 'x3 x4 x8', 'x3 x6 x12', 'x3 x6 x17', 'x1 x2 x7', 'x3 x4^2', 'x0 x6 x16', 'x1 x5 x10', 'x2 x3 x14', 'x0 x1 x4', 'x0 x2 x6', 'x12^2 x16', 'x0 x4 x15', 'x2 x11 x18', 'x8 x15 x17', 'x0 x8 x9', 'x1 x6 x11', 'x4 x12^2', 'x12 x14 x18', 'x4 x15 x17', 'x2 x3 x10', 'x11 x13 x18', 'x12^2', 'x7^2 x11', 'x2 x6 x15', 'x11 x15 x17', 'x15^3', 'x0 x8 x11', 'x4 x5 x14', 'x6 x14 x17', 'x4 x16^2', 'x2 x7 x12', 'x4 x6 x18', 'x8 x12 x15', 'x0 x1', 'x8 x16^2', 'x2 x4 x14', 'x11 x18^2', 'x2 x6 x10', 'x10 x11 x13', 'x8 x9 x15', 'x2 x4^2', 'x3 x4 x11', 'x10 x11 x15', 'x0 x4 x7', 'x1 x11 x15', 'x5 x6^2', 'x4 x6 x10', 'x0 x1 x7', 'x3 x5^2', 'x2 x6 x18', 'x0 x4 x5', 'x0 x3 x15', 'x4 x6 x14', 'x8^2 x15', 'x1 x4 x11', 'x2^2 x4', 'x1 x2 x9', 'x1 x4 x15', 'x1 x2 x5', 'x0^2 x6', 'x1 x3 x9', 'x12 x13 x16', 'x9 x17 x18', 'x5 x6 x12', 'x1 x18^2', 'x8 x14 x18', 'x2 x4 x15', 'x0 x1 x5', 'x4 x13 x16', 'x7 x9 x11', 'x2 x11^2', 'x3 x6 x8', 'x2 x8 x13', 'x1 x6 x7', 'x2 x6 x17', 'x12 x14 x17', 'x0 x4 x16', 'x0 x2 x4', 'x4 x6 x9', 'x1 x4 x8', 'x14^3', 'x5 x6 x10', 'x3 x6', 'x2 x3 x6', 'x2 x5 x15', 'x14 x15 x16', 'x1 x6', 'x3 x5 x6', 'x0 x6 x17', 'x0 x3 x17', 'x1 x3 x10', 'x2 x5 x9', 'x0 x2', 'x1 x2^2', 'x4 x9 x13', 'x3 x5 x11', 'x2 x4 x12', 'x3 x11^2', 'x9 x13 x17', 'x0 x5', 'x9 x11 x14', 'x0 x2 x3', 'x12 x13 x14', 'x5 x10', 'x7 x11 x13', 'x0 x4 x18', 'x0 x5 x12', 'x1 x3 x7', 'x1 x12 x14', 'x2 x5 x13', 'x4 x6^2', 'x0 x4', 'x2 x6 x9', 'x5 x6 x17', 'x4 x5 x16', 'x11^2 x12', 'x11^2 x18', 'x0 x1 x13', 'x5 x14 x15', 'x0 x6 x9', 'x3 x9 x11', 'x0 x5 x7', 'x4 x6', 'x0 x1 x18', 'x1 x3', 'x0 x1 x12', 'x3 x4 x13', 'x4 x6 x8', 'x8 x17 x18', 'x8^2 x17', 'x8 x15^2', 'x3 x5 x10', 'x1 x3 x15', 'x2 x11 x17', 'x2 x6 x16', 'x0 x3 x14', 'x0 x4 x6', 'x4 x9 x11', 'x7 x10 x13', 'x14 x18^2', 'x0 x2^2', 'x2^2 x5', 'x5 x6', 'x1 x9^2', 'x1 x3 x11', 'x2 x14 x15', 'x6 x11 x14', 'x12 x13 x17', 'x2 x10 x12', 'x3 x4 x9', 'x0 x5 x16', 'x11 x16 x18', 'x8 x12 x18', 'x13 x15^2', 'x3 x10^2', 'x1 x2 x18', 'x11 x12 x18', 'x0 x1 x16', 'x2 x3 x11', 'x3 x11 x15', 'x4 x5 x7', 'x11^2 x15', 'x2 x6', 'x1 x6 x16', 'x2 x9 x12', 'x1^2 x6', 'x0 x4 x11', 'x7 x18^2', 'x1 x5 x7', 'x1 x6 x13', 'x2 x4 x9', 'x11 x15^2', 'x0 x10 x16', 'x5 x6 x15', 'x2 x4 x11', 'x5 x11 x18', 'x1 x4^2', 'x4 x5 x10', 'x11^2 x14', 'x1 x2 x3', 'x4 x6 x13', 'x9^2 x14', 'x2 x6 x13', 'x15 x17', 'x0 x2 x8', 'x1 x5 x18', 'x1 x5 x12', 'x1 x2 x4', 'x1 x5^2', 'x4 x5 x13', 'x3 x5 x17', 'x8 x13 x18', 'x1 x2 x16', 'x1 x6 x10', 'x6 x11 x16', 'x2 x4 x8', 'x14 x16^2', 'x15 x18^2', 'x9 x11 x15', 'x9^2 x10', 'x0 x3 x4', 'x5 x6 x16', 'x8 x12 x14', 'x4 x9 x14', 'x10^2 x15', 'x4 x5 x8', 'x11 x14^2', 'x2 x5 x6', 'x2 x5 x16', 'x4 x5', 'x2 x6 x11', 'x3 x6 x11', 'x9^2 x15', 'x5 x6 x18', 'x16 x17 x18', 'x1 x12 x15', 'x3 x5 x14', 'x10 x11 x16', 'x2 x4 x16', 'x3 x4 x12', 'x10^2 x14', 'x1 x6 x8', 'x2 x13 x18', 'x16^3', 'x4 x5 x12', 'x0 x6 x18', 'x1 x5 x14', 'x4 x5 x15', 'x13 x16 x17', 'x0 x2 x10', 'x8^2 x11', 'x11 x14 x16', 'x4 x9^2', 'x4 x14 x17', 'x12^2 x17', 'x6 x11^2', 'x6 x8 x9', 'x7 x14 x17', 'x6 x7 x18', 'x1 x6^2', 'x0 x4 x17', 'x0 x5 x9', 'x3 x11 x14', 'x3 x12 x15', 'x8 x15 x16', 'x7 x16^2', 'x0 x6 x14', 'x2 x6 x7', 'x4 x6 x7', 'x1 x4 x18', 'x8^2 x14', 'x5 x6 x8', 'x14^2 x16', 'x3 x11 x13', 'x6^2 x14', 'x1 x2 x15', 'x2 x6^2', 'x11 x12 x14', 'x11 x14 x18', 'x13^2 x18', 'x2 x3 x16', 'x7 x10 x17', 'x1 x4 x16', 'x3 x5 x12', 'x15 x16^2', 'x3 x6^2', 'x3 x6 x9', 'x14 x15 x17', 'x1 x2 x14', 'x4 x5 x9', 'x3 x10 x17', 'x14^2 x17', 'x9 x16^2', 'x4 x8 x11', 'x10 x12 x13', 'x1 x5 x13', 'x1 x6 x12', 'x0 x1 x8', 'x0 x5 x17', 'x5 x6 x9', 'x13 x16 x18', 'x1 x2 x11', '1', 'x9 x15 x17', 'x0^2 x4', 'x14^2 x15', 'x15 x17 x18', 'x3 x4', 'x0 x3 x18', 'x15 x16 x17', 'x1 x5 x8', 'x3 x5 x15', 'x4 x6 x15', 'x1 x3 x4', 'x0 x1 x14', 'x10 x16 x18', 'x2 x4', 'x4 x6 x12', 'x9 x16 x18', 'x6 x7^2', 'x2 x3 x8', 'x0 x6 x13', 'x0 x3 x7', 'x4 x5 x11', 'x5 x6 x13', 'x9 x15^2']\n",
        "\n",
        "\n",
        "# Function for log transformation\n",
        "def log_transform(x):\n",
        "    return np.log1p(x)  # Adding 1 to ensure no issues with log(0)\n",
        "\n",
        "# Define the column transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('onehot', OneHotEncoder(), ['symboling']),           # One-hot encoding for 'symboling'\n",
        "        ('log', FunctionTransformer(log_transform), ['normalized-losses']), # Log transformation for 'normalized-losses'\n",
        "        ('scaler', StandardScaler(), [col for col in tX.columns if col not in ['symboling', 'normalized-losses']]) # Standard scaling for other columns\n",
        "    ]\n",
        ")\n",
        "\n",
        "tX = preprocessor.fit_transform(tX)\n",
        "vX = preprocessor.transform(vX)\n",
        "\n",
        "poly = PolynomialFeatures(3)\n",
        "tX = poly.fit_transform(tX)\n",
        "cols = poly.get_feature_names_out()\n",
        "tX = pd.DataFrame(tX, columns=cols)\n",
        "tX = tX.drop(cols_to_drop, axis=1)\n",
        "vX = poly.transform(vX)\n",
        "vX = pd.DataFrame(vX, columns=cols)\n",
        "vX = vX.drop(cols_to_drop, axis=1)\n",
        "\n",
        "m = MultiOutputRegressor(Ridge(random_state=42, alpha=0.0001))\n",
        "m.fit(tX, tY)\n",
        "y_pred = m.predict(vX)\n",
        "\n",
        "print(vX.shape)\n",
        "print(tX.shape)\n",
        "\n",
        "# In-sample R^2 score\n",
        "in_sample_r2 = m.score(tX, tY)\n",
        "print(f'In-sample R^2 = {in_sample_r2:.4f}')\n",
        "\n",
        "# Get predictions and R2 score for the validation set\n",
        "y_pred = m.predict(vX)\n",
        "validation_r2 = r2_score(DO_NOT_USE, y_pred)\n",
        "print(f'Validation R^2 = {validation_r2:.4f}')\n",
        "\n",
        "pY = pd.DataFrame(y_pred, index=vXcp.index, columns=YCols)\n",
        "ToCSV(pY, 'Auto_baseline')"
      ],
      "metadata": {
        "id": "_FWGriFHdou7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "722c9faa-6044-47f6-98d6-12ae26b10392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiOutputRegressor(estimator=Ridge(alpha=0.0001, random_state=42))"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiOutputRegressor(estimator=Ridge(alpha=0.0001, random_state=42))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiOutputRegressor</label><div class=\"sk-toggleable__content\"><pre>MultiOutputRegressor(estimator=Ridge(alpha=0.0001, random_state=42))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(alpha=0.0001, random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(alpha=0.0001, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300000, 924)\n",
            "(250000, 924)\n",
            "In-sample R^2 = 0.3810\n",
            "Validation R^2 = 0.3772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Finding insignificant feature names'''\n",
        " # Extracting coefficients for each target variable\n",
        "# coefficients = [estimator.coef_ for estimator in m.estimators_]\n",
        "\n",
        "# # Assuming your target has multiple columns like tY\n",
        "# target_columns = tY.columns\n",
        "\n",
        "# # Creating a DataFrame for better visualization\n",
        "# for i, target in enumerate(target_columns):\n",
        "#     coef_df = pd.DataFrame({\n",
        "#         'Feature': tX.columns,\n",
        "#         'Coefficient': coefficients[i]\n",
        "#     }).sort_values(by='Coefficient', ascending=False)\n",
        "\n",
        "#     print(f\"Coefficients for target variable {target}:\")\n",
        "#     print(coef_df)\n",
        "#     print(\"\\n\")\n",
        "\n",
        "# # Assuming you have already extracted the coefficients as before\n",
        "# threshold = 0.01  # Example threshold, adjust based on your data and model\n",
        "\n",
        "# # Identify non-significant features\n",
        "# insignificant_features = []\n",
        "# for coef in coefficients:\n",
        "#     # Assuming coef is a numpy array of coefficients\n",
        "#     insignificant_features.extend([tX.columns[i] for i in range(len(coef)) if abs(coef[i]) < threshold])\n",
        "\n",
        "# # Remove duplicates from the list\n",
        "# insignificant_features = list(set(insignificant_features))\n",
        "\n",
        "# # Drop non-significant features from datasets\n",
        "# tX_reduced = tX.drop(insignificant_features, axis=1)\n",
        "# vX_reduced = vX.drop(insignificant_features, axis=1)\n",
        "\n",
        "# # Re-train the model with reduced feature set\n",
        "# m_reduced = MultiOutputRegressor(Ridge(random_state=42))\n",
        "# m_reduced.fit(tX_reduced, tY)\n",
        "\n",
        "# # Evaluate the new model\n",
        "# y_pred_reduced = m_reduced.predict(vX_reduced)\n",
        "# validation_r2_reduced = r2_score(DO_NOT_USE, y_pred_reduced)\n",
        "# print(f'Reduced model validation R^2: {validation_r2_reduced:.4f}')\n",
        "\n",
        "# print(insignificant_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "GpIXi3G6iwyE",
        "outputId": "5674e1b4-220b-4939-9de0-bd2bcdbdb4f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Finding insignificant feature names'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(len(insignificant_features))"
      ],
      "metadata": {
        "id": "TJyrPI2DkPgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TAiB9yxwjTB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkz9ySewbTU-"
      },
      "source": [
        "# tX = tXcp\n",
        "# vX = vXcp\n",
        "\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler, PolynomialFeatures\n",
        "# from sklearn.linear_model import Ridge, LinearRegression\n",
        "# from sklearn.multioutput import MultiOutputRegressor\n",
        "# from sklearn.metrics import r2_score\n",
        "# from sklearn.svm import SVR\n",
        "# import numpy as np\n",
        "\n",
        "# # Function for log transformation\n",
        "# def log_transform(x):\n",
        "#     return np.log1p(x)  # Adding 1 to ensure no issues with log(0)\n",
        "\n",
        "# # Define the column transformer\n",
        "# preprocessor = ColumnTransformer(\n",
        "#     transformers=[\n",
        "#         ('onehot', OneHotEncoder(), ['symboling']),           # One-hot encoding for 'symboling'\n",
        "#         ('log', FunctionTransformer(log_transform), ['normalized-losses']), # Log transformation for 'normalized-losses'\n",
        "#         ('scaler', StandardScaler(), [col for col in tX.columns if col not in ['symboling', 'normalized-losses']]) # Standard scaling for other columns\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# # Define the pipeline steps including the preprocessor and the model\n",
        "# pipeline_steps = [\n",
        "#     ('preprocessor', preprocessor),\n",
        "#     ('poly_features', PolynomialFeatures(degree=3)),\n",
        "#     ('ridge_regression', MultiOutputRegressor(Ridge(random_state=0, alpha=0.0001))) #0.3749\n",
        "#     # ('linear_regression', MultiOutputRegressor(LinearRegression())) 0.3749\n",
        "#     # ('sv_regression',  MultiOutputRegressor(SVR(C=1.0, epsilon=0.2)))  // DOES NOT WORK!!!!!!!!!!!!!!!\n",
        "# ]\n",
        "\n",
        "# # Create the pipeline\n",
        "# pipeline = Pipeline(steps=pipeline_steps)\n",
        "\n",
        "# # Fit the pipeline on the training data\n",
        "# pipeline.fit(tX[:250000], tY[:250000])\n",
        "# # pipeline.fit(vX, DO_NOT_USE)\n",
        "# # In-sample R^2 score\n",
        "# # in_sample_r2 = pipeline.score(tX[:100000], tY[:100000])\n",
        "# # print(f'In-sample R^2 = {in_sample_r2:.4f}')\n",
        "# '''In-sample R^2 = 0.3860'''\n",
        "\n",
        "# # Get predictions and R2 score for the validation set\n",
        "# y_pred = pipeline.predict(vX)\n",
        "# validation_r2 = r2_score(DO_NOT_USE, y_pred)\n",
        "# print(f'Validation R^2 = {validation_r2:.4f}')\n",
        "\n",
        "# # Best score\n",
        "# '''0.34778582397226304''' #Quadratic\n",
        "# '''0.3711195460216124''' #Cubic // Obvs since morey\n",
        "# '''0.3723'''\n",
        "\n",
        "# '''0.3749'''\n",
        "# '''0.3752'''\n",
        "# '''0.3772''' #250k 58sec\n",
        "# pY = pd.DataFrame(y_pred, index=vXcp.index, columns=YCols)\n",
        "# ToCSV(pY, 'Auto_baseline')\n",
        "# '''0.3826 when trained on the real model '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''KNN'''\n",
        "\n",
        "# from sklearn.neighbors import KNeighborsRegressor\n",
        "# from sklearn.metrics import r2_score\n",
        "\n",
        "# # Define the range of k values to test\n",
        "# k_values = range(1, 21)\n",
        "\n",
        "# # Iterate over different values of k\n",
        "# for k in k_values:\n",
        "#     # Create a KNN regressor with the current value of k\n",
        "#     knn = MultiOutputRegressor(KNeighborsRegressor(n_neighbors=k))\n",
        "\n",
        "#     # Replace the model in the pipeline\n",
        "#     pipeline.steps[-1] = ('knn_regression', knn)\n",
        "\n",
        "#     # Fit the model\n",
        "#     pipeline.fit(tX[:100000], tY[:100000])\n",
        "\n",
        "#     # Predict on the validation set\n",
        "#     y_pred = pipeline.predict(vX)\n",
        "\n",
        "#     # Calculate the R2 score\n",
        "#     r2 = r2_score(DO_NOT_USE, y_pred)\n",
        "\n",
        "#     # Print the result\n",
        "#     print(f'R2 score for k={k}: {r2:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "TWMODFtPePPt",
        "outputId": "49016212-9778-4ec4-840b-2a91d475f47c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'KNN'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Cross-validation to find optimal alpha'''\n",
        "\n",
        "# from sklearn.metrics import r2_score\n",
        "\n",
        "# # Parameter grid to iterate over\n",
        "# alpha_values = [0.01, 0.05, 0.1, 0.2]\n",
        "\n",
        "# # Best score and parameters tracking\n",
        "# best_score = -np.inf\n",
        "# best_alpha = None\n",
        "\n",
        "# # Iterate over parameter grid\n",
        "# for alpha in alpha_values:\n",
        "#     # Set the model with the current parameter\n",
        "#     pipeline.set_params(ridge_regression__estimator__alpha=alpha)\n",
        "\n",
        "#     # Fit the model\n",
        "#     pipeline.fit(tX[:150000], tY[:150000])\n",
        "\n",
        "#     # Evaluate the model\n",
        "#     y_pred = pipeline.predict(vX)\n",
        "#     current_score = r2_score(DO_NOT_USE, y_pred)\n",
        "\n",
        "#     print(f\"Alpha: {alpha}, R2 Score: {current_score:.4f}\")\n",
        "\n",
        "#     # Update best parameters if current score is better\n",
        "#     if current_score > best_score:\n",
        "#         best_score = current_score\n",
        "#         best_alpha = alpha\n",
        "\n",
        "# # Print the best parameters and score\n",
        "# print(f\"Best Alpha: {best_alpha}, Best R2 Score: {best_score:.4f}\")\n",
        "\n",
        "# # Set the pipeline to the best parameters and fit on all available data\n",
        "# pipeline.set_params(ridge_regression__estimator__alpha=best_alpha)\n",
        "# pipeline.fit(tX[:150000], tY[:150000])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Uci5O9U3G8Ba",
        "outputId": "dcf11838-86ca-4dfe-8515-a6fb37ad60a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Cross-validation to find optimal alpha'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.ensemble import ExtraTreesRegressor  # or ExtraTreesClassifier for classification tasks\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# # Assuming tX is your training feature set and tY is the training target set\n",
        "# # Fit an ExtraTrees model\n",
        "# et_model = ExtraTreesRegressor(n_estimators=100, random_state=0)  # Adjust parameters as needed\n",
        "# et_model.fit(tX[:10000], tY[:10000])\n",
        "\n",
        "# # Get feature importances\n",
        "# feature_importances = et_model.feature_importances_\n",
        "\n",
        "# # Select top N important features (you can decide N based on your requirement)\n",
        "# N = 5  # For example, top 5 features\n",
        "# top_features = np.argsort(feature_importances)[-N:]\n",
        "\n",
        "# # Create new features based on interactions of the top features\n",
        "# for i in range(len(top_features)):\n",
        "#     for j in range(i + 1, len(top_features)):\n",
        "#         new_feature_name = f'interaction_{i}_{j}'\n",
        "#         tX[new_feature_name] = tX.iloc[:, top_features[i]] * tX.iloc[:, top_features[j]]\n",
        "\n",
        "# # Now tX contains the new interaction features\n"
      ],
      "metadata": {
        "id": "-A_bu5YkD-BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Fit the pipeline on the training data\n",
        "# pipeline.fit(tX[:10000], tY[:10000])\n",
        "\n",
        "# # Access the Ridge models inside the MultiOutputRegressor\n",
        "# ridge_models = pipeline.named_steps['ridge_regression'].estimators_\n",
        "\n",
        "# # Get feature names after polynomial features transformation (assuming non-categorical features are passed to it)\n",
        "# poly_transformer = PolynomialFeatures(degree=2)\n",
        "# non_categorical_cols = [col for col in tX.columns if col not in ['symboling', 'normalized-losses']]\n",
        "# poly_features = poly_transformer.fit_transform(tX[non_categorical_cols][:10000])\n",
        "# poly_feature_names = poly_transformer.get_feature_names_out(non_categorical_cols)\n",
        "\n",
        "# # For one-hot encoded features\n",
        "# one_hot_feature_names = pipeline.named_steps['preprocessor'].named_transformers_['onehot'].get_feature_names_out()\n",
        "\n",
        "# # Combine all feature names\n",
        "# all_feature_names = np.concatenate([one_hot_feature_names, ['normalized-losses'], poly_feature_names])\n",
        "\n",
        "# # Display coefficients for each target\n",
        "# for i, model in enumerate(ridge_models):\n",
        "#     print(f\"Target {i} Coefficients:\")\n",
        "#     for feature_name, coef in zip(all_feature_names, model.coef_):\n",
        "#         print(f\"{feature_name}: {coef}\")\n",
        "#     print(\"\\n\")\n",
        "\n",
        "# # Initialize a set to store features with coefficients less than 0.01\n",
        "# low_importance_features = set()\n",
        "\n",
        "# # Iterate through each model and their coefficients\n",
        "# for model in ridge_models:\n",
        "#     for feature_name, coef in zip(all_feature_names, model.coef_):\n",
        "#         if abs(coef) < 0.01:\n",
        "#             low_importance_features.add(feature_name)\n",
        "\n",
        "# # Convert the set to a sorted list for consistent ordering\n",
        "# low_importance_features_list = sorted(list(low_importance_features))\n",
        "\n",
        "# # Print the features\n",
        "# print(low_importance_features_list)\n",
        "\n",
        "# # Total number of features\n",
        "# total_feature_count = len(all_feature_names)\n",
        "# # Print the total number of features\n",
        "# print(f\"Total number of features: {total_feature_count}\")"
      ],
      "metadata": {
        "id": "2L3VvqEgoih2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References:**"
      ],
      "metadata": {
        "id": "pzBsjCvS_kEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Remember to cite your sources here as well! At the least, your textbook should be cited. Google Scholar allows you to effortlessly copy/paste an APA citation format for books and publications. Also cite StackOverflow, package documentation, and other meaningful internet resources to help your peers learn from these (and to avoid plagiarism claims)."
      ],
      "metadata": {
        "id": "2kr8Q-9T_nAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=5>‚åõ</font> <strong><font color=orange size=5>Do not exceed competition's runtime limit!</font></strong>\n",
        "\n",
        "<hr color=red>\n"
      ],
      "metadata": {
        "id": "DoF2GoB_QGw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmr.ShowTime()    # measure Colab's runtime. Do not remove. Keep as the last cell in your notebook."
      ],
      "metadata": {
        "id": "bD1sdgYbNWQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f444f22-2726-488f-cf0d-bd331718c174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runtime is 51 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nnK53pcbVY0"
      },
      "source": [
        "## üí°**Starter Ideas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn7E4huqbY-I"
      },
      "source": [
        "1. Tune model hyperparameters and try different allowed models\n",
        "1. Try to linear and non-linear feature normalization: shift/scale, log, divide features by features (investigate scatterplot matrix)\n",
        "1. Try higher order feature interactions and polynomial features on a small subsample. Then identify key features or select key principal components. The final model can be trained on a larger or even full training sample. You can use [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) to reduce the feature set\n",
        "1. Do a thorough EDA: look for feature augmentations that result in linear decision boundaries between pairs of classes.\n",
        "1. Evaluate predictions and focus on poorly predicted \"groups\":\n",
        "  1. Strongest errors. E.g. the model is very confident about the wrong label\n",
        "1. Do scatter plots show piecewise linear shape? Can a separate linear model be used on each support, or can the pattern be linearized via transformations?\n",
        "1. Try modeling each output separately from inputs or from a other modeled output\n",
        "1. Try stepwise selection and regularization and remove \"unimportant\" features from final model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ToCSV(DO_NOT_USE, 'Auto_baseline')"
      ],
      "metadata": {
        "id": "uZPw3K1ymkYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# '''Testing optimal sample size'''\n",
        "\n",
        "# from sklearn.model_selection import learning_curve\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# # Assuming tX is your training set and not the full dataset\n",
        "# n_train_samples = len(tX)\n",
        "\n",
        "# # Define the number of training samples to use for learning curves\n",
        "# # Generate, for example, 10 training sizes between 10% and 80% of the total number of training samples\n",
        "# train_sizes = np.linspace(int(0.1 * n_train_samples), int(0.8 * n_train_samples), 8, dtype=int)\n",
        "\n",
        "# # Calculate the learning curve\n",
        "# train_sizes, train_scores, validation_scores = learning_curve(\n",
        "#     estimator=pipeline,\n",
        "#     X=tX,\n",
        "#     y=tY,\n",
        "#     train_sizes=train_sizes,\n",
        "#     cv=5,  # 5-fold cross-validation\n",
        "#     scoring='r2',\n",
        "#     n_jobs=-1  # Use all CPU cores\n",
        "# )\n",
        "\n",
        "# # Calculate the mean and standard deviation for training and validation sets\n",
        "# train_scores_mean = np.mean(train_scores, axis=1)\n",
        "# train_scores_std = np.std(train_scores, axis=1)\n",
        "# validation_scores_mean = np.mean(validation_scores, axis=1)\n",
        "# validation_scores_std = np.std(validation_scores, axis=1)\n",
        "\n",
        "# # Plot the learning curves\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
        "# plt.plot(train_sizes, validation_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
        "\n",
        "# plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
        "# plt.fill_between(train_sizes, validation_scores_mean - validation_scores_std, validation_scores_mean + validation_scores_std, alpha=0.1, color=\"g\")\n",
        "\n",
        "# plt.title(\"Learning Curve\")\n",
        "# plt.xlabel(\"Training Set Size\")\n",
        "# plt.ylabel(\"R¬≤ Score\")\n",
        "# plt.legend(loc=\"best\")\n",
        "# plt.grid()\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "JJJKx960qktX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# '''10 regression each making preditions'''\n",
        "\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.base import clone\n",
        "\n",
        "# # Initialize KFold for 10 splits\n",
        "# kf = KFold(n_splits=10)\n",
        "\n",
        "# # Placeholder for combined predictions\n",
        "# combined_predictions = np.zeros(DO_NOT_USE.shape)\n",
        "\n",
        "# # Iterate over each fold\n",
        "# for train_index, _ in kf.split(tX):\n",
        "#     # Clone the pipeline to ensure a fresh model for each fold\n",
        "#     model = clone(pipeline)\n",
        "\n",
        "#     # Select the subset of the data for this fold\n",
        "#     tX_subset = tX.iloc[train_index]\n",
        "#     tY_subset = tY.iloc[train_index]\n",
        "\n",
        "#     # Fit the model on the subset\n",
        "#     model.fit(tX_subset, tY_subset)\n",
        "\n",
        "#     # Predict on the validation set and accumulate the predictions\n",
        "#     combined_predictions += model.predict(vX)\n",
        "\n",
        "# # Average the predictions over the 10 models\n",
        "# combined_predictions /= 10\n",
        "\n",
        "# # Calculate the R¬≤ score using combined predictions\n",
        "# combined_r2_score = r2_score(DO_NOT_USE, combined_predictions)\n",
        "# print(f'Combined R¬≤ score for averaged models: {combined_r2_score:.4f}')\n",
        "\n",
        "# '''Produces the same result ;('''"
      ],
      "metadata": {
        "id": "FtFF0RKntmVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Testing whether predicting separately is any better than predicting by itself '''\n",
        "\n",
        "# from sklearn.base import clone\n",
        "# from sklearn.linear_model import Ridge\n",
        "# from sklearn.metrics import r2_score\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "\n",
        "# # Assuming tX, tY, vX, and DO_NOT_USE are defined\n",
        "# # Clone the pipeline to use its preprocessing steps\n",
        "# individual_pipeline = clone(pipeline)\n",
        "# individual_pipeline.steps[-1] = ('ridge_regression', Ridge(random_state=0))  # Replace with single-output Ridge\n",
        "\n",
        "# # Placeholder for combined predictions\n",
        "# combined_predictions = np.zeros(DO_NOT_USE.shape)\n",
        "\n",
        "# # Train separate models for each target and predict\n",
        "# for i, column in enumerate(tY.columns):\n",
        "#     # Fit the model for each target column\n",
        "#     individual_pipeline.fit(tX, tY[column])\n",
        "\n",
        "#     # Predict on the validation set and store the predictions\n",
        "#     combined_predictions[:, i] = individual_pipeline.predict(vX)\n",
        "\n",
        "# # Convert combined predictions to a DataFrame for comparison\n",
        "# combined_predictions_df = pd.DataFrame(combined_predictions, columns=DO_NOT_USE.columns)\n",
        "\n",
        "# # Calculate and print the R¬≤ score using combined predictions\n",
        "# combined_r2_score = r2_score(DO_NOT_USE, combined_predictions_df)\n",
        "# print(f'Combined R¬≤ score for individual models: {combined_r2_score:.4f}')\n",
        "\n",
        "# # Assuming validation_r2 is the R¬≤ score from the MultiOutputRegressor\n",
        "# print(f'MultiOutputRegressor R¬≤ score: {validation_r2:.4f}')\n",
        "'''Seems like equivalent'''"
      ],
      "metadata": {
        "id": "qf-qNWuGxWUn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "511e48a3-eb66-475b-dbd3-62eb4588da45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Testing whether predicting separately is any better than predicting by itself '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Seems like equivalent'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Checks whether applying log improves R2 score'''\n",
        "\n",
        "# from itertools import combinations\n",
        "# from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "# from sklearn.linear_model import Ridge\n",
        "# from sklearn.multioutput import MultiOutputRegressor\n",
        "# from sklearn.metrics import r2_score\n",
        "# import numpy as np\n",
        "\n",
        "# def apply_log_transform_to_combinations(X_train, X_test, comb):\n",
        "#     \"\"\"\n",
        "#     Apply log transformation to the specified combination of columns.\n",
        "#     \"\"\"\n",
        "#     X_train_log = X_train.copy()\n",
        "#     X_test_log = X_test.copy()\n",
        "\n",
        "#     # Apply log transformation only to the specified columns\n",
        "#     for col in comb:\n",
        "#         if (X_train[col] > 0).all():  # Check if the column can be log-transformed\n",
        "#             X_train_log[col] = np.log1p(X_train[col])\n",
        "#             X_test_log[col] = np.log1p(X_test[col])\n",
        "\n",
        "#     return X_train_log, X_test_log\n",
        "\n",
        "# def evaluate_log_transforms(tX, vX, tY, y_true, columns_to_explore):\n",
        "#     \"\"\"\n",
        "#     Evaluate R¬≤ score improvements with log transformations on feature combinations.\n",
        "#     \"\"\"\n",
        "#     # Standardize the features\n",
        "#     sc = StandardScaler()\n",
        "#     tX_scaled = sc.fit_transform(tX)\n",
        "#     vX_scaled = sc.transform(vX)\n",
        "\n",
        "#     # Apply polynomial features\n",
        "#     poly = PolynomialFeatures(2)\n",
        "#     tX_poly = poly.fit_transform(tX_scaled)\n",
        "#     vX_poly = poly.transform(vX_scaled)\n",
        "\n",
        "#     # Fit the original model without log transformation\n",
        "#     original_model = MultiOutputRegressor(Ridge(random_state=0)).fit(tX_poly, tY)\n",
        "#     original_pred = original_model.predict(vX_poly)\n",
        "#     original_r2 = r2_score(y_true, original_pred)\n",
        "\n",
        "#     print(f'Original R¬≤ score: {original_r2:.4f}')\n",
        "\n",
        "#     # Check all combinations of the columns\n",
        "#     for i in range(1, len(columns_to_explore) + 1):\n",
        "#         for comb in combinations(columns_to_explore, i):\n",
        "#             # Apply log transformation to the current combination\n",
        "#             tX_log, vX_log = apply_log_transform_to_combinations(tX, vX, comb)\n",
        "\n",
        "#             # Standardize the features\n",
        "#             tX_log_scaled = sc.fit_transform(tX_log)\n",
        "#             vX_log_scaled = sc.transform(vX_log)\n",
        "\n",
        "#             # Apply polynomial features\n",
        "#             tX_log_poly = poly.fit_transform(tX_log_scaled)\n",
        "#             vX_log_poly = poly.transform(vX_log_scaled)\n",
        "\n",
        "#             # Fit the model with the transformed columns\n",
        "#             model = MultiOutputRegressor(Ridge(random_state=0)).fit(tX_log_poly, tY)\n",
        "#             y_pred = model.predict(vX_log_poly)\n",
        "\n",
        "#             # Calculate and print the R¬≤ score\n",
        "#             r2 = r2_score(y_true, y_pred)\n",
        "#             print(f'R¬≤ score with log transform on {comb}: {r2:.4f}')\n",
        "\n",
        "# # Usage\n",
        "# # Replace 'tX', 'vX', 'tY', and 'y_true' with the actual datasets\n",
        "# # 'columns_to_explore' should be the list of column names you want to consider for log transformation\n",
        "# tXsend = tX.drop(columns='Symboling')\n",
        "# evaluate_log_transforms(tX, vX, tY, DO_NOT_USE, tXsend)\n",
        "'''Takes too long to run'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ETFn5OBqiu4o",
        "outputId": "74a53885-0b25-4e03-d375-b650f92a2411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Checks whether applying log improves R2 score'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Takes too long to run'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}